{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5486b49-d4fd-4cc2-b28f-430b9ace8aa8",
   "metadata": {},
   "source": [
    "# Document Summarization Using GPT-3.5-turbo and PDF Processing\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This Jupyter notebook outlines a comprehensive approach to automatically summarizing large PDF documents using advanced NLP techniques and GPT-3.5-turbo, OpenAI's state-of-the-art language model. The process involves extracting text from a PDF file, segmenting the text into coherent sections, and then utilizing machine learning models to summarize and weave these sections into a cohesive narrative.\n",
    "\n",
    "The goal of this project is to demonstrate how to handle and transform large volumes of text into concise summaries that maintain the core information and context. This is particularly useful in academic, legal, or corporate settings where quickly understanding large documents is crucial.\n",
    "\n",
    "## Workflow Summary\n",
    "\n",
    "1. **PDF Text Extraction**: Extract text from a PDF file, handling various formatting and structure to ensure high-quality text retrieval.\n",
    "2. **Text Segmentation**: Identify logical sections within the extracted text, using titles or headings as delimiters to enhance the relevance of the segments.\n",
    "3. **Text Summarization**: Apply NLP techniques to summarize each identified section. This step involves generating embeddings for text segments, clustering them to find similar themes, and summarizing each cluster.\n",
    "4. **Generating Cohesive Narrative**: Enhance the flow between summaries using GPT-3.5-turbo to generate transitional texts that link the summaries in a coherent and fluid manner.\n",
    "5. **Output as PDF**: Finally, compile the generated summaries into a single document, formatted as a PDF, ready for distribution or archiving.\n",
    "\n",
    "## Author\n",
    "* Nizar ZEROUALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e220ef-f4a5-46fc-9328-34d173082bf9",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "Before we dive into the implementation details of our document summarization project, let's first import all the necessary libraries:\n",
    "\n",
    "- **PyMuPDF (`fitz`)**: This library will be used for reading and extracting text from PDF files. It provides robust capabilities for handling various PDF content formats.\n",
    "- **Sentence Transformers**: A library built on top of Hugging Face's transformers. It's designed for generating sentence embeddings, which are crucial for understanding and comparing text segments semantically.\n",
    "- **Scikit-learn (`KMeans`)**: From this popular machine learning library, we use the KMeans clustering algorithm to group text segments into clusters based on their semantic similarity.\n",
    "- **NumPy**: Essential for handling large arrays and matrices of numerical data, which is fundamental for any data processing in machine learning tasks.\n",
    "- **re (Regular Expressions)**: This module is utilized for text processing tasks, specifically for identifying and extracting text patterns which is vital in segmenting text based on titles or subtitles.\n",
    "- **FPDF**: A library for assembling and saving our summaries into a PDF format, allowing for easy distribution and archiving of the final document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef2bc24d-8472-4cc8-b5de-b9b385263928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed4b0b-2bb2-4ab0-892c-a39f4253c48e",
   "metadata": {},
   "source": [
    "## Setup for OpenAI API Integration\n",
    "\n",
    "This section of the notebook establishes our connection to the OpenAI API, which is essential for accessing GPT-3.5 to perform advanced natural language processing tasks, including generating transitions and summarizing text:\n",
    "\n",
    "- **os**: This module will be used for environment management, especially for safely handling API keys.\n",
    "- **openai**: Directly imported from the OpenAI Python client library, this module enables us to interact with OpenAI's powerful GPT models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7466ba38-9089-4f37-bca1-7fd4af87108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # Replace 'your_openai_api_key' with your actual OpenAI API key\n",
    "    api_key=\"your_openai_api_key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dcd5e1-a5cd-4e75-97dd-66d681904819",
   "metadata": {},
   "source": [
    "## 1. PDF Text Extraction\n",
    "\n",
    "In this section, we focus on extracting textual content from a PDF document, which is the first critical step in our document summarization process. Efficient and accurate extraction of text from PDFs is essential as it forms the basis for all subsequent processing and analysis steps. \n",
    "\n",
    "We will use the `PyMuPDF` library, known in our code as `fitz`, to handle this task. This library provides robust support for interacting with PDF files, enabling us to read pages and extract text efficiently. Here's what the upcoming code will accomplish:\n",
    "\n",
    "- **Open the PDF file**: We'll load the PDF document into our Python environment.\n",
    "- **Read and Extract Text**: Iterate through each page of the PDF to gather all textual content.\n",
    "- **Store and Print Text**: The extracted text will be stored for further processing and a snippet will be printed to verify the extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57dcb153-224a-409b-8670-a2002f7f1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    # Open the provided PDF file\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    \n",
    "    # Iterate through each page of the PDF\n",
    "    for page in doc:\n",
    "        # Extract text from the page\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Close the document\n",
    "    doc.close()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623353e-e222-46f5-9256-1dab35c4684a",
   "metadata": {},
   "source": [
    "## 2. Text Segmentation\n",
    "\n",
    "After extracting the text from the PDF, our next task is to segment it into logical sections. Proper segmentation is crucial as it allows us to process and summarize each part of the document more effectively, especially in large and complex documents where content varies significantly from one section to another.\n",
    "\n",
    "### How We Segment the Text\n",
    "We use a function called `split_into_sections`, which employs regular expressions to identify patterns that typically denote the start of a new section. These patterns include:\n",
    "\n",
    "- Numbers indicating sections (e.g., `1`, `2.1`, etc.).\n",
    "- Capital letters possibly indicating appendices or major section starts (e.g., `A`, `B.2`).\n",
    "- These are often followed by titles in all caps or initial caps that help in recognizing the beginning of a section.\n",
    "\n",
    "**Regex Approach**: \n",
    "The function defines a regex pattern that matches lines which are likely to be titles based on the above criteria. This helps in splitting the document text at these points:\n",
    "\n",
    "- The regex pattern used is `r'(\\n(\\d+|\\d+\\.\\d+|[A-Z]|\\w\\.\\d+)\\s*\\n[A-Z])'`, which looks for new lines followed by a pattern of numbers or letters that are typical of titles or headers, followed by a new line starting with a capital letter.\n",
    "- We then create a list of start indices for each match found by the regex, which marks where each new section begins.\n",
    "- The text is split into sections using these indices, ensuring each segment captures a complete thought or topic as indicated by the document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58fbd626-bb9d-4b3c-90e9-b8fdb6c8bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sections(text):\n",
    "    # Regex pattern to match the described subtitle formats\n",
    "    pattern = r'(\\n(\\d+|\\d+\\.\\d+|[A-Z]|\\w\\.\\d+)\\s*\\n[A-Z])'\n",
    "    sections = []\n",
    "    start_indices = [0] + [match.start(1) for match in re.finditer(pattern, text)]\n",
    "    \n",
    "    # Split the text at each start index\n",
    "    start_indices.append(len(text))  # Append the end of the text to handle the last section\n",
    "    for i in range(len(start_indices) - 1):\n",
    "        section = text[start_indices[i]:start_indices[i+1]].strip()\n",
    "        if section:\n",
    "            sections.append(section)\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31441a-bada-47cb-b97b-61d331afc221",
   "metadata": {},
   "source": [
    "## 3. Text Summarization\n",
    "\n",
    "Once we have segmented the text into logical sections, the next step in our text summarization pipeline is to convert these text segments into a numerical format that machine learning models can process. This is achieved through the generation of text embeddings.\n",
    "\n",
    "### What are Text Embeddings?\n",
    "Text embeddings are vector representations of text where words or phrases with similar meanings have a similar representation. By converting text into embeddings, we can perform various types of semantic analyses, such as clustering similar texts together based on their content.\n",
    "\n",
    "### Using Sentence Transformers for Embeddings\n",
    "In this section, we utilize the `SentenceTransformer` library, which is specifically designed for generating high-quality sentence embeddings. The model we use is `all-MiniLM-L6-v2`:\n",
    "\n",
    "- **Model Choice**: `all-MiniLM-L6-v2` is a lightweight, yet powerful model trained for generating embeddings that capture the semantic meaning of sentences efficiently.\n",
    "- **Process**: We pass our text segments into the model's `encode` method, which returns a list of embeddings. Each embedding is a vector that numerically represents the corresponding text segment.\n",
    "\n",
    "The embeddings generated here will be used in subsequent steps for clustering the text segments based on their semantic similarity, enabling us to summarize content that discusses similar topics.\n",
    "\n",
    "### Implementation Details\n",
    "The function `generate_embeddings` takes a list of text segments as input and returns their corresponding embeddings. This simplifies downstream tasks like clustering and summarization, as dealing with numerical data is often more straightforward and effective in machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f75a95f9-af02-4da8-b8d4-da74d60ad478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_segments):\n",
    "    # Code to generate embeddings for text segments\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model for generating embeddings\n",
    "    embeddings = model.encode(text_segments)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b516b-5614-4896-ae23-587121572a52",
   "metadata": {},
   "source": [
    "### Clustering Text Sections for Summarization\n",
    "In our text summarization workflow, one key step is to effectively group similar text segments into clusters. This step is crucial because it allows us to manage and summarize large documents more efficiently by focusing on representative segments from each cluster. Here is an overview of the clustering process implemented in the `cluster_segments` function:\n",
    "\n",
    "### Purpose of Clustering\n",
    "The primary goal of clustering in this context is to:\n",
    "\n",
    "Reduce Redundancy: By clustering similar text segments together, we can summarize only one segment from each cluster, reducing repetitive information in the final summary.\n",
    "Enhance Diversity: It ensures that the summarized content covers a broad range of topics or themes presented in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d40ca80-f459-4bcc-9390-38f6f0662140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_segments(embeddings):\n",
    "    # Code to cluster embeddings\n",
    "    n_clusters = min(250, len(embeddings))  # Define the number of clusters\n",
    "    clustering_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_labels = clustering_model.labels_\n",
    "    return cluster_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad08c5-6d03-446b-b24d-096d27cfd68d",
   "metadata": {},
   "source": [
    "### Concatenating Text Sections Within Clusters\n",
    "For our text summarization workflow, after segmenting and clustering text sections based on their semantic similarities, it is essential to manage the text within these clusters effectively. The `concatenate_cluster_samples` function addresses this by concatenating all text sections within each identified cluster. This step prepares the text for more comprehensive processing, such as summarization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "979a9f55-18d8-4477-9ced-69ca463fb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_cluster_samples(cluster_labels, text_sections):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    cluster_content = defaultdict(list)\n",
    "\n",
    "    # Group all sections by their cluster labels\n",
    "    for label, section in zip(cluster_labels, text_sections):\n",
    "        cluster_content[label].append(section)\n",
    "\n",
    "    # Concatenate all sections in each cluster into a single string\n",
    "    concatenated_sections = []\n",
    "    for sections in cluster_content.values():\n",
    "        concatenated_sections.append(\" \".join(sections))\n",
    "\n",
    "    return concatenated_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b5e4f-971f-4bee-9d15-41c6ca8ce55d",
   "metadata": {},
   "source": [
    "## Text Summarization with GPT-3.5-Turbo\n",
    "In our document processing workflow, one of the key functionalities is the summarization of text segments. This is carried out by the `summarize_text` function, which leverages the capabilities of OpenAI's GPT-3.5-turbo model. Here's a detailed look at how this function operates and its significance in the overall process:\n",
    "\n",
    "### Purpose of the Summarize Text Function\n",
    "The `summarize_text` function is designed to condense lengthy text segments into concise, informative summaries. This helps in distilling essential information from large volumes of text, making it more manageable and easier to understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2d285392-9d4e-4f12-97e7-ca664bff6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(segment):\n",
    "    # Code to summarize a text segment \n",
    "    prompt=(f\"Convert the following into a concise, factual summary without introductory phrases. The convertion should be narrated. \"\n",
    "              f\"Focus on key information and outcomes:\\n\\n{segment}\")\n",
    "    try:\n",
    "        # Make an API call to OpenAI's GPT-3.5-turbo model\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",  \n",
    "        \n",
    "            max_tokens=150,  # You can adjust this based on how concise you want the summary to be\n",
    "            temperature=0.2  # Controls randomness: lower values make the output more deterministic\n",
    "        )\n",
    "        # Extract the text summary from the response\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error in summarization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d3213-36b6-4f49-8e79-18ebb96f4ed2",
   "metadata": {},
   "source": [
    "### Estimating Token Count for Text\n",
    "Before processing text segments for summarization or any other NLP tasks, it's crucial to estimate how many tokens (words and punctuation marks) they contain. This estimation helps manage API usage, particularly when interacting with services that have limits on token counts per request, like OpenAI's GPT models. The `estimate_token_count` function provides a simple yet effective method for this estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aabf7d93-5b94-4cc3-8212-c751bf45adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_token_count(text):\n",
    "    # Rough estimation based on whitespace and common punctuation\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b06b74-f05f-45c5-8e38-4d4ff31027e1",
   "metadata": {},
   "source": [
    "## 4. Generating Cohesive Narrative\n",
    "In the process of compiling a comprehensive document from segmented summaries, it's crucial to ensure that the narrative flows logically and smoothly from one section to the next. The `create_fluid_summary` function is designed to achieve this by generating cohesive transitions between consecutive summaries, using OpenAI's GPT model. This approach enhances the readability and continuity of the final document, making it appear as a unified narrative rather than a collection of disjointed sections.\n",
    "\n",
    "### Function Overview\n",
    "The `create_fluid_summary` function integrates individual summaries into a seamless narrative by:\n",
    "\n",
    "* **Appending each summary to a collective text**\n",
    "* **Dynamically generating and inserting transition sentences between summaries using GPT-3.5-turbo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "66705647-1a65-4a2f-bc30-f0d5b8f4d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fluid_summary(summaries):\n",
    "\n",
    "    full_text = []\n",
    "\n",
    "    for i in range(len(summaries) - 1):\n",
    "        # Append the current summary\n",
    "        full_text.append(summaries[i])\n",
    "        \n",
    "        # Create the prompt for transition\n",
    "        prompt = (\n",
    "            f\"Create a smooth transition sentence between the following two sections:\\n\\n\"\n",
    "            f\"---\\nSection {i+1}:\\n{summaries[i]}\\n\\n\"\n",
    "            f\"---\\nSection {i+2}:\\n{summaries[i+1]}\\n\\n\"\n",
    "            \"Answer only with the transition :\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Request the transition from GPT-3.5-turbo\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "            \n",
    "                model=\"gpt-3.5-turbo\",\n",
    "\n",
    "                max_tokens=50,  # Adjust based on how long you expect transitions to be\n",
    "                temperature=0.4,  # Adjust for more creative transitions\n",
    "            )\n",
    "            transition = response.choices[0].message.content.strip()\n",
    "            full_text.append(transition)  # Append the generated transition\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while generating transition: {e}\")\n",
    "\n",
    "    # Append the last summary since it has no subsequent summary to transition to\n",
    "    full_text.append(summaries[-1])\n",
    "\n",
    "    # Join all parts into a single cohesive narrative\n",
    "    return \"\\n\\n\".join(full_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a19b7-f412-434c-bcd3-d148ef1990bc",
   "metadata": {},
   "source": [
    "## 5. Output as PDF\n",
    "Once we have our cohesive narrative or document summary, the next step is to preserve and share it in a universally accessible format. The save_text_as_pdf function serves this purpose by converting the final text into a PDF file. This functionality not only facilitates easy distribution but also ensures the content is presented in a professional format suitable for reading and printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "82c78f21-96a9-4d38-94c7-7d218e0dcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_as_pdf(text, filename=\"Summary.pdf\"):\n",
    "    \n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=8)\n",
    "    pdf.add_font(\"Arial\", \"\", \"arial.ttf\", uni=True)  # Ensure Unicode support for non-ASCII characters\n",
    "\n",
    "    # Add text to PDF\n",
    "    pdf.multi_cell(0, 10, text)  # Width = 0 means the cell is extended to the right margin\n",
    "\n",
    "    # Save the PDF to a file\n",
    "    pdf.output(filename)\n",
    "    print(f\"PDF successfully saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c7953-593b-486b-8cb9-d611cb42ab22",
   "metadata": {},
   "source": [
    "## Main Function Overview\n",
    "\n",
    "The `main` function orchestrates a comprehensive sequence of operations to process a PDF document into a coherent summary. Below is a step-by-step explanation of each component involved in this process:\n",
    "\n",
    "- **Extract Text**: Retrieves all readable content from the specified PDF file.\n",
    "\n",
    "- **Segment Text**: Divides the extracted text into sections based on predefined patterns, facilitating better organization of the content.\n",
    "\n",
    "- **Generate Embeddings**: Converts each section into numerical representations (embeddings) that capture their semantic meaning.\n",
    "\n",
    "- **Cluster Text Sections**: Groups similar text sections into clusters to consolidate related content, enhancing summarization efficiency.\n",
    "\n",
    "- **Concatenate Cluster Samples**: Merges all text sections within each cluster to form a comprehensive view of each cluster’s content.\n",
    "\n",
    "- **Generate Summaries**: Summarizes the concatenated text of each cluster, producing a concise overview of its content.\n",
    "\n",
    "- **Create Cohesive Narrative**: Integrates all individual summaries into a single fluid narrative using transitions generated between them.\n",
    "\n",
    "- **Output as PDF**: Saves the final cohesive narrative into a PDF document, ready for distribution or archival.\n",
    "\n",
    "### Function Execution Details\n",
    "- The function monitors and manages the number of text sections, embeddings, and cluster labels processed, providing insight into the data handling stages.\n",
    "- It controls API usage by implementing pauses when the estimated token count approaches a preset limit, ensuring efficient use of resources without exceeding API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "79082fb7-dde7-4647-a14d-f4fcf625f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_path = './2303.08774v6.pdf'\n",
    "    full_text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    sections = split_into_sections(full_text)\n",
    "    \n",
    "    print(\"sections\")\n",
    "    #for section in sections:  \n",
    "    #    print(section)\n",
    "    #    print(\"-------------------------------------------------------\")\n",
    "    \n",
    "    print(len(sections))\n",
    "\n",
    "    embeddings = generate_embeddings(sections)\n",
    "    print(\"embeddings\")\n",
    "    print(len(embeddings))\n",
    "    #print(embeddings[:10])\n",
    "    \n",
    "    cluster_labels = cluster_segments(embeddings)\n",
    "    print(\"cluster labels\")\n",
    "    print(len(cluster_labels))\n",
    "    #print(cluster_labels)\n",
    "    \n",
    "    concatenated_samples = concatenate_cluster_samples(cluster_labels, sections)\n",
    "    print(\"Concatenated cluster samples: \", len(concatenated_samples))\n",
    "    print(len(concatenated_samples[-1]))\n",
    "\n",
    "    summaries = []\n",
    "    pause_duration = 100  # Pause duration in seconds\n",
    "    tokens_used = 0\n",
    "    max_tokens_per_minute = 30000\n",
    "\n",
    "    for sample in concatenated_samples:\n",
    "        print(\"Sample\" + str(concatenated_samples.index(sample)))\n",
    "        #print(sample)\n",
    "        estimated_tokens = estimate_token_count(sample) + 150  # Add expected output tokens\n",
    "\n",
    "        if tokens_used + estimated_tokens > max_tokens_per_minute:\n",
    "            print(\"Approaching token limit, pausing...\")\n",
    "            time.sleep(pause_duration)\n",
    "            tokens_used = 0  # Reset tokens after pause\n",
    "\n",
    "        summary = summarize_text(sample)\n",
    "        print(summary)\n",
    "        summaries.append(summary)\n",
    "        tokens_used += estimated_tokens  # Update tokens used\n",
    "\n",
    "    \n",
    "    final_summary = create_fluid_summary(summaries)\n",
    "    print(\"Final Summary :\", final_summary)\n",
    "    save_text_as_pdf(final_summary, filename='Summary_GPT-4_Technical_Report.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0a482-a7cf-4c02-b707-de5e27248fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728e23e-0481-46a7-8c16-49e83276bd9a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we have demonstrated a robust process for extracting text from a PDF document, segmenting the text, generating semantic embeddings, and clustering the text for efficient summarization. We successfully transformed these clusters into concatenated samples, from which we generated concise summaries. These summaries were then integrated into a cohesive narrative through the use of intelligent transition sentences generated by OpenAI's GPT model.\n",
    "\n",
    "The final narrative was outputted as a formatted PDF document, making it suitable for a wide range of applications including academic reviews, business reporting, or any scenario requiring quick digestion of large documents.\n",
    "\n",
    "### Achievements\n",
    "- **Efficient Text Handling**: Managed to efficiently process large volumes of text by leveraging clustering to reduce redundancy and focus on diversity.\n",
    "- **NLP Techniques**: Utilized state-of-the-art NLP techniques and AI technologies to generate meaningful and context-aware summaries.\n",
    "- **Automation and Scalability**: Established a workflow that can be automated and scaled for handling multiple documents or larger datasets.\n",
    "\n",
    "### Future Directions\n",
    "- **Improvement of Text Segmentation**: Further refine the text segmentation to capture more nuanced distinctions between sections and improve the accuracy of clustering.\n",
    "- **Enhanced Summary Personalization**: Develop methods to tailor summaries more closely to specific user needs or preferences.\n",
    "- **Broader Language Support**: Extend the model's capabilities to include multiple languages, increasing its applicability in global contexts.\n",
    "\n",
    "This notebook stands as a testament to the power of combining traditional data processing techniques with advanced AI-driven tools to enhance information accessibility and usability. The methods and processes outlined here are adaptable to various content types and industries, promising broad utility in the data-driven world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d01e7-40fe-4754-97b2-72211bfbd069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
